# -*- coding: utf-8 -*-
"""Fauzeeya_classification-using-keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GPJ1n3b5dPNSexVvFpLSn6tpmfR35gxa

# **Importing libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder
import seaborn as sns
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, LSTM

from google.colab import files
uploaded = files.upload()

"""# **Data loading and visualising.**"""

data = pd.read_csv('Crop_recommendation.csv')
data.head()

data['label'].unique()

"""# **Dividing data into training and testing parts**"""

X = data.iloc[:, 0:-1].values # iloc is function for indexing of dataframes.
Y = data.label.values

# OneHotEncoding
encoder = OneHotEncoder() # using encoding of class_type as this is a multi class problem.
Y = encoder.fit_transform(Y.reshape(-1,1)).toarray() # fitting our data to encoder.

X, Y

# train_test_split is a function used to split our data for training and testing purpose.
x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.2)

"""# **MLP**"""

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(7,22), max_iter=500, activation='relu')
mlp.fit(x_train, y_train)
pred = mlp.predict(x_test)

y_pred_con = mlp.predict(x_test)
y_pred, y_correct = [], []

for i in y_test:
    y_correct.append(np.argmax(i))
for j in y_pred_con:
    y_pred.append(np.argmax(j))
    
pred_df = pd.DataFrame()
pred_df['Pred_class'] = y_pred
pred_df['Correct_class'] = y_correct
pred_df

import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
import tensorflow as tf
print(classification_report(y_pred, y_correct))
cm = tf.math.confusion_matrix(y_pred, y_correct)

plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import mean_absolute_error, max_error, mean_squared_error, mean_absolute_percentage_error
print(accuracy_score(y_pred, y_correct))
print(mean_absolute_error(y_pred, y_correct))
print(mean_squared_error(y_pred, y_correct))
print(max_error(y_pred, y_correct))

"""# **Creating ANN Model**"""

# creating model
model = Sequential()

# adding hidden layers with number of units and activation function.
model.add(Dense(units = 64, activation = 'relu', input_dim = 7)) #hiddenlayer1 with and extra parameter input dimensions which is 16 in out case that is no. of features in training data.
model.add(Dense(units = 32, activation = 'relu')) #hiddenlayer2
model.add(Dense(units = 22, activation = 'sigmoid')) #outputlayer

# compiling our model.
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy']) #metrics are the list of parameters on which we test our model like accuracy.

# fitting data to train our model and then validating score with validation_data.
model.fit(x_train, y_train, epochs=40, batch_size=8, validation_data=(x_test, y_test))

# printing score with evaluate
print(model.evaluate(x_test, y_test)[1])

y_pred_con = model.predict(x_test)
y_pred, y_correct = [], []

for i in y_test:
    y_correct.append(np.argmax(i))
for j in y_pred_con:
    y_pred.append(np.argmax(j))
    
pred_df = pd.DataFrame()
pred_df['Pred_class'] = y_pred
pred_df['Correct_class'] = y_correct
pred_df

confusion_matrix(y_pred, y_correct)

import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
import tensorflow as tf
print(classification_report(y_pred, y_correct))
cm = tf.math.confusion_matrix(y_pred, y_correct)

plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import mean_absolute_error, max_error, mean_squared_error, mean_absolute_percentage_error
print(accuracy_score(y_pred, y_correct))
print(mean_absolute_error(y_pred, y_correct))
print(mean_squared_error(y_pred, y_correct))
print(max_error(y_pred, y_correct))

"""# **CREATING RNN MODEL**"""

from keras.layers import Flatten
model = Sequential()

# adding hidden layers with number of units and activation function.
model.add(LSTM(units = 64, activation = 'relu',input_shape=(7,1))) #hiddenlayer1 with and extra parameter input dimensions which is 16 in out case that is no. of features in training data.
model.add(Flatten())
model.add(Dense(units = 32, activation = 'relu')) #hiddenlayer2
model.add(Dense(units = 22, activation = 'sigmoid')) #outputlayer

# compiling our model.
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy']) #metrics are the list of parameters on which we test our model like accuracy.

# fitting data to train our model and then validating score with validation_data.
model.fit(x_train, y_train, epochs=40, batch_size=8, validation_data=(x_test, y_test))

y_pred_con = model.predict(x_test)
y_pred, y_correct = [], []

for i in y_test:
    y_correct.append(np.argmax(i))
for j in y_pred_con:
    y_pred.append(np.argmax(j))
    
pred_df = pd.DataFrame()
pred_df['Pred_class'] = y_pred
pred_df['Correct_class'] = y_correct
pred_df

import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics
import tensorflow as tf
print(classification_report(y_pred, y_correct))
cm = tf.math.confusion_matrix(y_pred, y_correct)

plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import mean_absolute_error, max_error, mean_squared_error, mean_absolute_percentage_error
print(accuracy_score(y_pred, y_correct))
print(mean_absolute_error(y_pred, y_correct))
print(mean_squared_error(y_pred, y_correct))
print(max_error(y_pred, y_correct))

